{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fc10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97279b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MisogynyDataset(Dataset):\n",
    "    def __init__(self, data, label_map, transform=None):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.label_map = label_map\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transform,\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = self.label_map[row[\"image_label\"]]\n",
    "        caption = row[\"image_caption\"]\n",
    "        return image, caption, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca932355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MisogynyDataLoader:\n",
    "    def __init__(self, csv_file=\"data_csv.csv\", batch_size=16, test_size=0.2, random_state=42,\n",
    "                 train_transform=None, test_transform=None, num_workers=0, pin_memory=False):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        label_map = {\"kitchen\":0, \"shopping\":1, \"working\":2, \"leadership\":3}\n",
    "\n",
    "        train_df, test_df = train_test_split(\n",
    "            data,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            shuffle=True,\n",
    "            stratify=data[\"image_label\"]\n",
    "        )\n",
    "\n",
    "        self.train_dataset = MisogynyDataset(train_df, label_map, transform=train_transform)\n",
    "        self.test_dataset = MisogynyDataset(test_df, label_map, transform=test_transform)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                       num_workers=num_workers, pin_memory=pin_memory)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                      num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8acb050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model_bert.eval()\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_bert(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sentence_embeddings = (token_embeddings * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ec7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenClipVitEmbedder(nn.Module):\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__()\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
    "            model_name=\"ViT-B-32\", pretrained=\"openai\"\n",
    "        )\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, image_tensor):\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(image_tensor)\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801f82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCALayer(nn.Module):\n",
    "    def __init__(self, mean, components):\n",
    "        super().__init__()\n",
    "        mean = torch.tensor(mean, dtype=torch.float32)\n",
    "        components = torch.tensor(components, dtype=torch.float32)\n",
    "        self.register_buffer(\"mean\", mean)\n",
    "        self.register_buffer(\"weight\", components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, D)\n",
    "        x = x - self.mean\n",
    "        return torch.matmul(x, self.weight.T)  # (B, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f3a0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_embeddings(dataloader, text_model, image_model, device):\n",
    "    text_embeddings = []\n",
    "    image_embeddings = []\n",
    "\n",
    "    for images, captions, _ in dataloader:\n",
    "        captions = list(captions)\n",
    "        with torch.no_grad():\n",
    "            text_emb = text_model(captions).to(\"cpu\")\n",
    "            image_emb = image_model(images).to(\"cpu\")\n",
    "        text_embeddings.append(text_emb.numpy())\n",
    "        image_embeddings.append(image_emb.numpy())\n",
    "\n",
    "    text_embeddings = np.vstack(text_embeddings)\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    return text_embeddings, image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f144f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 2201.86it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "/Users/vishnurchityala/FILES/USERS/VISHNU/PROJECTS/ONGOING-PROJECTS/graph-networks/venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataloaders = MisogynyDataLoader()\n",
    "train_loader = dataloaders.train_loader\n",
    "\n",
    "text_model = BERTEmbedder().to(device)\n",
    "image_model = OpenClipVitEmbedder(device=device)\n",
    "\n",
    "text_train_emb, image_train_emb = collect_embeddings(train_loader, text_model, image_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_text = PCA(n_components=400, svd_solver=\"full\")\n",
    "pca_text.fit(text_train_emb)\n",
    "pca_image = PCA(n_components=400, svd_solver=\"full\")\n",
    "pca_image.fit(image_train_emb)\n",
    "\n",
    "np.save(\"bert_pca_mean.npy\", pca_text.mean_)\n",
    "np.save(\"bert_pca_components.npy\", pca_text.components_)\n",
    "np.save(\"clip_pca_mean.npy\", pca_image.mean_)\n",
    "np.save(\"clip_pca_components.npy\", pca_image.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb4d4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pca_layer = PCALayer(pca_text.mean_, pca_text.components_)\n",
    "clip_pca_layer = PCALayer(pca_image.mean_, pca_image.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d38b759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text PCA shape: torch.Size([16, 400])\n",
      "Image PCA shape: torch.Size([16, 400])\n"
     ]
    }
   ],
   "source": [
    "for images, captions, _ in train_loader:\n",
    "    text_emb = text_model(captions)\n",
    "    text_emb_pca = bert_pca_layer(text_emb)\n",
    "\n",
    "    image_emb = image_model(images)\n",
    "    image_emb_pca = clip_pca_layer(image_emb)\n",
    "\n",
    "    print(\"Text PCA shape:\", text_emb_pca.shape)     # (B, 400)\n",
    "    print(\"Image PCA shape:\", image_emb_pca.shape)   # (B, 400)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
