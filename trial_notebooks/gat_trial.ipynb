{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea13cc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishnurchityala/FILES/USERS/VISHNU/PROJECTS/ONGOING-PROJECTS/graph-networks/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import open_clip\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527b913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MisogynyDataset(Dataset):\n",
    "    def __init__(self, data, label_map, transform=None):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = self.label_map[row[\"image_label\"]]\n",
    "        caption = row[\"image_caption\"]\n",
    "        return image, caption, label\n",
    "\n",
    "class MisogynyDataLoader:\n",
    "    def __init__(self, csv_file=\"data_csv.csv\", batch_size=16, test_size=0.2, random_state=42,\n",
    "                 train_transform=None, test_transform=None, num_workers=0, pin_memory=False):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        label_map = {\"kitchen\":0, \"shopping\":1, \"working\":2, \"leadership\":3}\n",
    "\n",
    "        train_df, test_df = train_test_split(\n",
    "            data,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            shuffle=True,\n",
    "            stratify=data[\"image_label\"]\n",
    "        )\n",
    "\n",
    "        self.train_dataset = MisogynyDataset(train_df, label_map, transform=train_transform)\n",
    "        self.test_dataset = MisogynyDataset(test_df, label_map, transform=test_transform)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                       num_workers=num_workers, pin_memory=pin_memory)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                      num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2bbfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model_bert.eval()\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_bert(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sentence_embeddings = (token_embeddings * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "\n",
    "class OpenClipVitEmbedder(nn.Module):\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__()\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
    "            model_name=\"ViT-B-32\", pretrained=\"openai\"\n",
    "        )\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, image_tensor):\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(image_tensor)\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01693b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDALayer(nn.Module):\n",
    "    def __init__(self, mean, coef):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", torch.tensor(mean, dtype=torch.float32))\n",
    "        self.register_buffer(\"weight\", torch.tensor(coef, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - self.mean\n",
    "        x = torch.matmul(x, self.weight.T)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c11dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-contained GAT module with stored training graph.\n",
    "    Forward function can take new embeddings and return contextualized embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_features, k=20, hidden_dim=32, out_dim=64, heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"node_features\", torch.tensor(node_features, dtype=torch.float32))\n",
    "        self.k = k\n",
    "        self.edge_index = self._build_knn_graph(node_features, k)\n",
    "        in_dim = node_features.shape[1]\n",
    "        self.gat1 = GATConv(in_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_dim*heads, out_dim, heads=1, concat=False, dropout=dropout)\n",
    "\n",
    "    def _build_knn_graph(self, embeddings, k):\n",
    "        embeddings_norm = normalize(embeddings, axis=1)\n",
    "        knn = kneighbors_graph(embeddings_norm, n_neighbors=k, mode='connectivity', include_self=True)\n",
    "        knn = 0.5 * (knn + knn.T)\n",
    "        coo = knn.tocoo()\n",
    "        return torch.tensor([coo.row, coo.col], dtype=torch.long)\n",
    "\n",
    "    def forward(self, new_node_features, k=None):\n",
    "        \"\"\"\n",
    "        new_node_features: tensor of shape (num_new_nodes, feature_dim)\n",
    "        Returns: contextualized embeddings for new nodes\n",
    "        \"\"\"\n",
    "        k = k or self.k\n",
    "        device = self.node_features.device\n",
    "        training_nodes = self.node_features\n",
    "        new_nodes = new_node_features.to(device)\n",
    "\n",
    "        # Combine training nodes + new nodes\n",
    "        all_nodes = torch.cat([training_nodes, new_nodes], dim=0)\n",
    "\n",
    "        # Build edges for new nodes\n",
    "        num_training = training_nodes.shape[0]\n",
    "        num_new = new_nodes.shape[0]\n",
    "        edge_rows = []\n",
    "        edge_cols = []\n",
    "\n",
    "        # For each new node, find top-k neighbors in training nodes\n",
    "        sim = cosine_similarity(new_nodes.cpu().numpy(), training_nodes.cpu().numpy())  # (num_new, num_training)\n",
    "        for i in range(num_new):\n",
    "            topk_idx = np.argsort(sim[i])[-k:]\n",
    "            new_idx = num_training + i\n",
    "            edge_rows.extend([new_idx]*k + topk_idx.tolist())\n",
    "            edge_cols.extend(topk_idx.tolist() + [new_idx]*k)  # bidirectional\n",
    "\n",
    "        # Combine with existing edges\n",
    "        existing_edges = self.edge_index\n",
    "        new_edges = torch.tensor([edge_rows, edge_cols], dtype=torch.long).to(device)\n",
    "        combined_edge_index = torch.cat([existing_edges.to(device), new_edges], dim=1)\n",
    "\n",
    "        # Apply GAT layers\n",
    "        x = self.gat1(all_nodes, combined_edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat2(x, combined_edge_index)\n",
    "\n",
    "        # Return only new nodes' embeddings\n",
    "        return x[-num_new:]\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            \"gat_state_dict\": self.state_dict(),\n",
    "            \"node_features\": self.node_features,\n",
    "            \"edge_index\": self.edge_index\n",
    "        }, path)\n",
    "        print(f\"GraphModule saved at {path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, k=20, hidden_dim=32, out_dim=64, heads=4, dropout=0.2, device=None):\n",
    "        checkpoint = torch.load(path, map_location=device or \"cpu\")\n",
    "        node_features = checkpoint[\"node_features\"].cpu().numpy()\n",
    "        model = cls(node_features, k=k, hidden_dim=hidden_dim, out_dim=out_dim, heads=heads, dropout=dropout)\n",
    "        model.load_state_dict(checkpoint[\"gat_state_dict\"])\n",
    "        if device:\n",
    "            model.to(device)\n",
    "        print(f\"GraphModule loaded from {path}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef49356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_embeddings(dataloader, text_model, image_model, device):\n",
    "    text_embeddings = []\n",
    "    image_embeddings = []\n",
    "\n",
    "    for images, captions, _ in dataloader:\n",
    "        captions = list(captions)\n",
    "        with torch.no_grad():\n",
    "            text_emb = text_model(captions).to(\"cpu\")\n",
    "            image_emb = image_model(images).to(\"cpu\")\n",
    "        text_embeddings.append(text_emb.numpy())\n",
    "        image_embeddings.append(image_emb.numpy())\n",
    "\n",
    "    text_embeddings = np.vstack(text_embeddings)\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    return text_embeddings, image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5069e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1416.26it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "/Users/vishnurchityala/FILES/USERS/VISHNU/PROJECTS/ONGOING-PROJECTS/graph-networks/venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule saved at graph_module.pth\n",
      "GraphModule loaded from graph_module.pth\n",
      "Contextualized embedding shape: torch.Size([1, 64])\n",
      "Embedding vector: tensor([[-0.7221, -0.6699,  0.3172, -0.3804,  1.4258,  0.2117,  0.0096, -0.3153,\n",
      "         -1.2866, -2.6263, -1.5839, -1.7813, -0.5408,  1.2618,  0.6201, -0.5782,\n",
      "          0.1267,  0.1445,  0.6805,  0.4058, -1.4602, -0.3465,  0.7490, -0.0436,\n",
      "          1.4061,  0.1863,  0.9807, -0.7317, -2.6265,  1.5070, -0.9574, -1.9253,\n",
      "         -0.7596,  1.0118,  1.2317,  0.4074,  0.7122,  1.4086, -0.5095, -1.5471,\n",
      "          1.5757,  0.1525, -1.3396,  2.3373,  0.3033, -0.7901, -1.6310, -0.1629,\n",
      "          0.8914,  0.2795, -0.9303, -0.6516,  0.3851, -1.8809,  0.3712,  0.4218,\n",
      "         -1.7391,  1.4416, -1.0993,  1.7109,  0.6776,  2.2506, -2.2052,  0.8949]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tt/h58zm7ld5jx4b9zpy0bhrg9w0000gn/T/ipykernel_76437/2656155061.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  return torch.tensor([coo.row, coo.col], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "dataloaders = MisogynyDataLoader()\n",
    "train_loader = dataloaders.train_loader\n",
    "\n",
    "text_model = BERTEmbedder().to(device)\n",
    "image_model = OpenClipVitEmbedder(device=device)\n",
    "\n",
    "text_train_emb, image_train_emb = collect_embeddings(train_loader, text_model, image_model, device)\n",
    "lda_mean = np.load(\"weights/combined_lda_mean.npy\")\n",
    "lda_coef = np.load(\"weights/combined_lda_coef.npy\")\n",
    "combined_lda_layer = LDALayer(lda_mean, lda_coef)\n",
    "\n",
    "combined_raw = np.concatenate([text_train_emb, image_train_emb], axis=1)\n",
    "combined_tensor = torch.tensor(combined_raw, dtype=torch.float32)\n",
    "combined_lda_emb = combined_lda_layer(combined_tensor).numpy()\n",
    "\n",
    "gat_module = GraphModule(node_features=combined_lda_emb, k=20, hidden_dim=32, out_dim=64)\n",
    "\n",
    "gat_module.save(\"graph_module.pth\")\n",
    "\n",
    "gat_module_loaded = GraphModule.load(\"graph_module.pth\", device=device)\n",
    "\n",
    "new_emb = np.random.rand(1, combined_lda_emb.shape[1]).astype(np.float32)\n",
    "new_emb_tensor = torch.tensor(new_emb)\n",
    "\n",
    "contextualized_emb = gat_module_loaded(new_emb_tensor, k=5)\n",
    "print(\"Contextualized embedding shape:\", contextualized_emb.shape)\n",
    "print(\"Embedding vector:\", contextualized_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
