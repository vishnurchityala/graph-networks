{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6517a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishnurchityala/FILES/USERS/VISHNU/PROJECTS/ONGOING-PROJECTS/graph-networks/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "import open_clip\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75fb27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MisogynyDataset(Dataset):\n",
    "    def __init__(self, data, label_map, transform=None):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = self.label_map[row[\"image_label\"]]\n",
    "        caption = row[\"image_caption\"]\n",
    "        return image, caption, label\n",
    "\n",
    "class MisogynyDataLoader:\n",
    "    def __init__(self, csv_file=\"data_csv.csv\", batch_size=16, test_size=0.2, random_state=42,\n",
    "                 train_transform=None, test_transform=None, num_workers=0, pin_memory=False):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        label_map = {\"kitchen\":0, \"shopping\":1, \"working\":2, \"leadership\":3}\n",
    "\n",
    "        train_df, test_df = train_test_split(\n",
    "            data,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            shuffle=True,\n",
    "            stratify=data[\"image_label\"]\n",
    "        )\n",
    "\n",
    "        self.train_dataset = MisogynyDataset(train_df, label_map, transform=train_transform)\n",
    "        self.test_dataset = MisogynyDataset(test_df, label_map, transform=test_transform)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                       num_workers=num_workers, pin_memory=pin_memory)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                      num_workers=num_workers, pin_memory=pin_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaad35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model_bert.eval()\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_bert(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sentence_embeddings = (token_embeddings * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "\n",
    "class OpenClipVitEmbedder(nn.Module):\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__()\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
    "            model_name=\"ViT-B-32\", pretrained=\"openai\"\n",
    "        )\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, image_tensor):\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(image_tensor)\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0406141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDALayer(nn.Module):\n",
    "    def __init__(self, mean, coef):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", torch.tensor(mean, dtype=torch.float32))\n",
    "        self.register_buffer(\"weight\", torch.tensor(coef, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - self.mean\n",
    "        x = torch.matmul(x, self.weight.T)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944bfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Module: k-NN graph creation, GAT layers, training, and save/load weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim=32, out_dim=64, heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_dim*heads, out_dim, heads=1, concat=False, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def create_knn_graph(embeddings, k=20):\n",
    "        embeddings_norm = normalize(embeddings, axis=1)\n",
    "        knn = kneighbors_graph(embeddings_norm, n_neighbors=k, mode='connectivity', include_self=True)\n",
    "        knn = 0.5 * (knn + knn.T)\n",
    "        coo = knn.tocoo()\n",
    "        edge_index = torch.tensor([coo.row, coo.col], dtype=torch.long)\n",
    "        return edge_index\n",
    "\n",
    "    def save_weights(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"GAT weights saved at {path}\")\n",
    "\n",
    "    def load_weights(self, path, map_location=None):\n",
    "        self.load_state_dict(torch.load(path, map_location=map_location))\n",
    "        print(f\"GAT weights loaded from {path}\")\n",
    "\n",
    "    def train_gat(self, x, edge_index, labels, mask=None, lr=0.005, weight_decay=5e-4, epochs=200, verbose=True):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        if mask is None:\n",
    "            mask = torch.ones(len(labels), dtype=torch.bool)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = self.forward(x, edge_index)\n",
    "            loss = F.cross_entropy(out[mask], labels[mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if verbose and epoch % 20 == 0:\n",
    "                pred = out.argmax(dim=1)\n",
    "                acc = (pred[mask] == labels[mask]).float().mean().item()\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Train Acc: {acc:.4f}\")\n",
    "        \n",
    "        return self.forward(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0811c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_embeddings(dataloader, text_model, image_model, device):\n",
    "    text_embeddings = []\n",
    "    image_embeddings = []\n",
    "\n",
    "    for images, captions, _ in dataloader:\n",
    "        captions = list(captions)\n",
    "        with torch.no_grad():\n",
    "            text_emb = text_model(captions).to(\"cpu\")\n",
    "            image_emb = image_model(images).to(\"cpu\")\n",
    "        text_embeddings.append(text_emb.numpy())\n",
    "        image_embeddings.append(image_emb.numpy())\n",
    "\n",
    "    text_embeddings = np.vstack(text_embeddings)\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    return text_embeddings, image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16739a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 2164.86it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "/Users/vishnurchityala/FILES/USERS/VISHNU/PROJECTS/ONGOING-PROJECTS/graph-networks/venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloaders = MisogynyDataLoader()\n",
    "train_loader = dataloaders.train_loader\n",
    "\n",
    "text_model = BERTEmbedder().to(device)\n",
    "image_model = OpenClipVitEmbedder(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41197f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_emb, image_train_emb = collect_embeddings(train_loader, text_model, image_model, device)\n",
    "labels = np.array([label for _, _, label in dataloaders.train_dataset])\n",
    "num_classes = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc30b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mean = np.load(\"weights/combined_lda_mean.npy\")\n",
    "lda_coef = np.load(\"weights/combined_lda_coef.npy\")\n",
    "combined_lda_layer = LDALayer(lda_mean, lda_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c6cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_raw = np.concatenate([text_train_emb, image_train_emb], axis=1)\n",
    "combined_tensor = torch.tensor(combined_raw, dtype=torch.float32)\n",
    "combined_lda_emb = combined_lda_layer(combined_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09273b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tt/h58zm7ld5jx4b9zpy0bhrg9w0000gn/T/ipykernel_72369/2467775775.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  edge_index = torch.tensor([coo.row, coo.col], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "edge_index = GraphModule.create_knn_graph(combined_lda_emb, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb813565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created successfully!\n",
      "Node features shape: (1704, 4)\n",
      "Edge index shape: torch.Size([2, 40146])\n"
     ]
    }
   ],
   "source": [
    "np.save(\"graph_node_features.npy\", combined_lda_emb)\n",
    "np.save(\"graph_edge_index.npy\", edge_index.numpy())\n",
    "np.save(\"graph_labels.npy\", labels)\n",
    "\n",
    "print(\"Graph created successfully!\")\n",
    "print(\"Node features shape:\", combined_lda_emb.shape)\n",
    "print(\"Edge index shape:\", edge_index.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
